{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acb246e1",
   "metadata": {},
   "source": [
    "## Writing to a csv file\n",
    "Import modules and locations of data needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c77b6b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #So we can store data in DataFrames (an object type that makes for easier analysis)\n",
    "import numpy as np #used for calculations\n",
    "import uproot3 #allows us to accesss the data within the files we are using\n",
    "import matplotlib.pyplot as plt #used for plotting graphs\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b29910",
   "metadata": {},
   "source": [
    "Data from: https://atlas-opendata.web.cern.ch/atlas-opendata/samples/2020/exactly2lep/MC/\n",
    "\n",
    "open MM800sig file that will be used to grab the names and locations of all the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8863805",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_306109.dmV_Zll_MET40_DM1_MM800.exactly2lep.root')[\"mini\"].keys()\n",
    "#holds column name data in byte form (for reading in data)\n",
    "\n",
    "\n",
    "relevant_data = ['mcWeight','lep_pt','lep_eta','lep_phi','lep_E','lep_charge','met_et','met_phi','jet_pt','jet_eta','jet_phi',\n",
    "                 'jet_E','XSection','SumWeights','jet_MV2c10','scaleFactor_PILEUP','scaleFactor_ELE','scaleFactor_MUON',\n",
    "                 'scaleFactor_LepTRIGGER']\n",
    "# these are the headings of data variables that we will be looking at (note that there are fewer than before to decrease file size)\n",
    "#I think we discussed adding in a couple more? truth matched/tight id and the systematics one\n",
    "\n",
    "relevant_data_locations = [3,18,19,20,21,23,30,31,33,34,35,36,68,69,40,4,5,6,10]\n",
    "#these are the numbers correseponding to locations of the variables within the ROOT files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf7bfd8",
   "metadata": {},
   "source": [
    "### Open signal files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1649dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MM700sig_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    MM700sig_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_305711.dmV_Zll_MET40_DM1_MM700.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "MM700sig_df = pd.DataFrame(MM700sig_dict)\n",
    "\n",
    "MM800sig_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    MM800sig_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_306109.dmV_Zll_MET40_DM1_MM800.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "MM800sig_df = pd.DataFrame(MM800sig_dict)\n",
    "\n",
    "MM2000sig_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    MM2000sig_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_303514.dmV_Zll_MET40_DM1_MM2000.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "MM2000sig_df = pd.DataFrame(MM2000sig_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04641257",
   "metadata": {},
   "source": [
    "### Open background files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df2dae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ZZ background MC\n",
    "llvv_dict = {}\n",
    "#llvv_open = \n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    llvv_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_363492.llvv.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "llvv_df = pd.DataFrame(llvv_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0222a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WZ background MC\n",
    "lllv_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    lllv_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_363491.lllv.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "lllv_df = pd.DataFrame(lllv_dict)\n",
    "    \n",
    "lvvv_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    lvvv_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_363493.lvvv.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "lvvv_df = pd.DataFrame(lvvv_dict)\n",
    "    \n",
    "WlvZqq_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    WlvZqq_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_363489.WlvZqq.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "WlvZqq_df = pd.DataFrame(WlvZqq_dict)\n",
    "   \n",
    "WqqZll_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    WqqZll_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_363358.WqqZll.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "WqqZll_df = pd.DataFrame(WqqZll_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4a11fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top background MC\n",
    "single_top_tchan_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    single_top_tchan_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_410011.single_top_tchan.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "single_top_tchan_df = pd.DataFrame(single_top_tchan_dict)\n",
    "\n",
    "single_antitop_tchan_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    single_antitop_tchan_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_410012.single_antitop_tchan.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "single_antitop_tchan_df = pd.DataFrame(single_antitop_tchan_dict)\n",
    "\n",
    "single_top_wtchan_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    single_top_wtchan_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_410013.single_top_wtchan.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "single_top_wtchan_df = pd.DataFrame(single_top_wtchan_dict)\n",
    "\n",
    "single_antitop_wtchan_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    single_antitop_wtchan_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_410014.single_antitop_wtchan.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "single_antitop_wtchan_df = pd.DataFrame(single_antitop_wtchan_dict)\n",
    "\n",
    "single_top_schan_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    single_top_schan_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_410025.single_top_schan.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "single_top_schan_df = pd.DataFrame(single_top_schan_dict)\n",
    "\n",
    "single_antitop_schan_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    single_antitop_schan_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_410026.single_antitop_schan.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "single_antitop_schan_df = pd.DataFrame(single_antitop_schan_dict)\n",
    "\n",
    "ttbar_lep_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    ttbar_lep_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_410000.ttbar_lep.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "ttbar_lep_df = pd.DataFrame(ttbar_lep_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f29796c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ztt (Ztautau) background MC\n",
    "Ztautau_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    Ztautau_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_361108.Ztautau.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "Ztautau_df = pd.DataFrame(Ztautau_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9777696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WW background MC\n",
    "WpqqWmlv_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    WpqqWmlv_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_363359.WpqqWmlv.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "WpqqWmlv_df = pd.DataFrame(WpqqWmlv_dict)\n",
    "\n",
    "WplvWmqq_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    WplvWmqq_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_363360.WplvWmqq.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "WplvWmqq_df = pd.DataFrame(WplvWmqq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca49b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "#W+jets background MC\n",
    "Wplusenu_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    Wplusenu_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_361100.Wplusenu.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "Wplusenu_df = pd.DataFrame(Wplusenu_dict)\n",
    "\n",
    "Wplusmunu_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    Wplusmunu_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_361101.Wplusmunu.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "Wplusmunu_df = pd.DataFrame(Wplusmunu_dict)\n",
    "\n",
    "Wplustaunu_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    Wplustaunu_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_361102.Wplustaunu.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "Wplustaunu_df = pd.DataFrame(Wplustaunu_dict)\n",
    "\n",
    "Wminusenu_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    Wminusenu_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_361103.Wminusenu.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "Wminusenu_df = pd.DataFrame(Wminusenu_dict)\n",
    "\n",
    "Wminusmunu_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    Wminusmunu_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_361104.Wminusmunu.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "Wminusmunu_df = pd.DataFrame(Wminusmunu_dict)\n",
    "\n",
    "Wminustaunu_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    Wminustaunu_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_361105.Wminustaunu.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "Wminustaunu_df = pd.DataFrame(Wminustaunu_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13def1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ttV background MC\n",
    "ttW_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    ttW_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_410155.ttW.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "ttW_df = pd.DataFrame(ttW_dict)\n",
    "    \n",
    "ttee_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    ttee_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_410218.ttee.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "ttee_df = pd.DataFrame(ttee_dict)\n",
    "    \n",
    "ttmumu_dict = {}\n",
    "counter = 0\n",
    "for i in relevant_data_locations:\n",
    "    ttmumu_dict[relevant_data[counter]] = uproot3.open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\mc_410219.ttmumu.exactly2lep.root')[\"mini\"].array(keys[i]) \n",
    "    counter+=1\n",
    "ttmumu_df = pd.DataFrame(ttmumu_dict)\n",
    "\n",
    "\n",
    "\n",
    "#######################################Changing the SumWeights and XSection on the files########################################\n",
    "ttW_sumW = []\n",
    "for i in ttW_df['SumWeights']:\n",
    "    ttW_sumW.append(4075279.75386)\n",
    "del ttW_df['SumWeights']\n",
    "ttW_df['SumWeights'] = ttW_sumW\n",
    "\n",
    "ttW_Xsec = []   \n",
    "for i in ttW_df['XSection']:\n",
    "    ttW_Xsec.append(0.60084912)\n",
    "    #ttW_df['XSection'] = ttW_df['XSection'].replace([i],0.60084912)\n",
    "del ttW_df['XSection']\n",
    "ttW_df['XSection'] = ttW_Xsec\n",
    "    \n",
    "    \n",
    "\n",
    "ttee_sumW = []\n",
    "for i in ttee_df['SumWeights']:\n",
    "    ttee_sumW.append(51968.9384584)\n",
    "    #ttee_df['SumWeights'] = ttee_df['SumWeights'].replace([i],51968.9384584)\n",
    "del ttee_df['SumWeights']\n",
    "ttee_df['SumWeights'] = ttee_sumW\n",
    "\n",
    "ttee_Xsec = []\n",
    "for i in ttee_df['XSection']:\n",
    "    ttee_Xsec.append(0.0412888)\n",
    "    #ttee_df['XSection'] = ttee_df['XSection'].replace([i],0.0412888)\n",
    "del ttee_df['XSection']\n",
    "ttee_df['XSection'] = ttee_Xsec\n",
    "\n",
    "\n",
    "\n",
    "ttmumu_sumW = []\n",
    "for i in ttmumu_df['SumWeights']:\n",
    "    ttmumu_sumW.append(52007.5311319)\n",
    "    #ttmumu_df['SumWeights'] = ttmumu_df['SumWeights'].replace([i],52007.5311319)\n",
    "del ttmumu_df['SumWeights']\n",
    "ttmumu_df['SumWeights'] = ttmumu_sumW\n",
    "    \n",
    "ttmumu_Xsec = []\n",
    "for i in ttmumu_df['XSection']:\n",
    "    ttmumu_Xsec.append(0.04129216)\n",
    "    #ttmumu_df['XSection'] = ttmumu_df['XSection'].replace([i],0.04129216)\n",
    "del ttmumu_df['XSection']\n",
    "ttmumu_df['XSection'] = ttmumu_Xsec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b99086",
   "metadata": {},
   "source": [
    "### Computing Weights and adding them to dataframes\n",
    "\n",
    "The below cell is used to calculate the weight of each event (aka row) in a dataframe, and it is possible that there may be a mistake here leading to errors later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4850c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE BELOW LIST IS USED AS FOLLOWS:\n",
    "\n",
    "#ADD THE NAMES OF THE DATAFRAMES THAT HAVE BEEN MADE (I.E. THE DATAFRAMES CORRESPONDING TO THE CURRENTLY OPEN FILE - THE REASON\n",
    "#WHY ALL THE DATAFRAME NAMES ARE COMMENTED OUT IS AS IT IS DIFFICULT TO HAVE THEM ALL OPEN AT ONCE, SO YOU OPEN SOME AT A TIME\n",
    "#AND WRITE THE NAMES OF THOSE DATAFRAMES BELOW)\n",
    "\n",
    "#RUN THIS CODE CELL TO CALCULATE THE WEIGHT\n",
    "\n",
    "df_names = [ttW_df, ttee_df, ttmumu_df]#llvv_df, lllv_df, lvvv_df, WlvZqq_df, WqqZll_df, single_top_tchan_df, \n",
    "            #single_antitop_tchan_df, single_top_wtchan_df, single_antitop_wtchan_df, single_top_schan_df, \n",
    "            #single_antitop_schan_df, ttbar_lep_df, Ztautau_df, WpqqWmlv_df, WplvWmqq_df, Wplusenu_df, Wplusmunu_df, Wplustaunu_df,\n",
    "            #Wminusenu_df, Wminusmunu_df, Wminustaunu_df, ttW_df, ttee_df, ttmumu_df]#[MM700sig_df, MM800sig_df, MM2000sig_df]\n",
    "            #raw_data_df,  Zee_df, Zmumu_df,\n",
    "\n",
    "#IN ORDER TO FIND THE TOTAL WEIGHT WE WILL NEED TO DO CALCULATIONS ON ALL THE DATA. THUS WE WILL MAKE A FUNCTION THAT CAN BE\n",
    "#APPLIED TO ALL DATA USING NP.VECTORISE\n",
    "\n",
    "def add_xsecW(dfname):\n",
    "    dfname['XSectionWeight'] = 10*1000*dfname['XSection']/dfname['SumWeights'] \n",
    "#10 refers to amount of data we have(10 fb-1), *1000 converts from fb-1 to pb-1\n",
    "\n",
    "for i in df_names:\n",
    "    add_xsecW(i)\n",
    "    \n",
    "    \n",
    "def add_Weight(dfname):    \n",
    "    dfname['Weight'] = dfname['XSectionWeight']*dfname['mcWeight']*dfname['scaleFactor_PILEUP']*dfname['scaleFactor_ELE']*dfname['scaleFactor_MUON']*dfname['scaleFactor_LepTRIGGER']\n",
    "\n",
    "for i in df_names:\n",
    "    add_Weight(i)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27036f57",
   "metadata": {},
   "source": [
    "Now that the weight has been calculated, these need to be added to the list of variables that we are using. This is done below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24510089",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_relevant_data = ['mcWeight','lep_pt','lep_eta','lep_phi','lep_E','lep_charge','met_et','met_phi','jet_pt','jet_eta','jet_phi',\n",
    "                 'jet_E','XSection','SumWeights','jet_MV2c10','scaleFactor_PILEUP','scaleFactor_ELE','scaleFactor_MUON',\n",
    "                 'scaleFactor_LepTRIGGER','XSectionWeight','Weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc4a4db",
   "metadata": {},
   "source": [
    "### Performing cuts on data to reduce file size\n",
    "\n",
    "Note that if the csv being made will contain events from multiple ROOT files, the below cell will need to be re-run multiple times, and the label of the dataframe at the end will need to be changed each time. Then, the dataframes can be combined and put into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "caa2fdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ttW_df, ttee_df, ttmumu_df\n",
    "initial_frame = ttmumu_df #write name of dataframe corresponding to each file\n",
    "    \n",
    "    \n",
    "#Cut 1:  𝐩𝑡  of leptons above 20 GeV,  𝐩𝑡  of leading lepton above 30 GeV  \n",
    "desired_indexes = []\n",
    "for i in range(len(initial_frame['lep_pt'])):\n",
    "    if (initial_frame['lep_pt'][i][0])/1000 > 30 and (initial_frame['lep_pt'][i][1])/1000 > 20: \n",
    "        #if leading lepton has p_t above 30 GeV and second lepton has p_t above 20 GeV\n",
    "        desired_indexes.append(i)\n",
    "leppt_cut_df = pd.DataFrame()#make a new dataframe to hold data after the cut\n",
    "for i in new_relevant_data:\n",
    "    kept_data = []\n",
    "    for j in desired_indexes:\n",
    "        kept_data.append(initial_frame[i][j])\n",
    "    leppt_cut_df[i] = kept_data\n",
    "\n",
    "        \n",
    "#Cut 2: ∆R less than 1.8\n",
    "desired_indexes = []\n",
    "for i in range(len(leppt_cut_df['lep_eta'])):\n",
    "    delta_eta = leppt_cut_df['lep_eta'][i][0] - leppt_cut_df['lep_eta'][i][1] # Δη between the 2 objects\n",
    "    delta_phi = leppt_cut_df['lep_phi'][i][0] - leppt_cut_df['lep_phi'][i][1] # Δϕ between the 2 objects\n",
    "    if delta_phi >= np.pi:\n",
    "        delta_phi -= 2*np.pi # use π periodicity to get number between -π and π\n",
    "    elif delta_phi < -np.pi: \n",
    "        delta_phi += 2*np.pi # use π periodicity to get number between -π and π\n",
    "    if np.sqrt(delta_eta**2 + delta_phi**2) < 1.8: # check if ∆R for this object is below our desired threshold\n",
    "        desired_indexes.append(i)\n",
    "delR_cut_df = pd.DataFrame()#make a new dataframe to hold data after the cut\n",
    "for i in new_relevant_data:\n",
    "    kept_data = []\n",
    "    for j in desired_indexes:\n",
    "        kept_data.append(leppt_cut_df[i][j])\n",
    "    delR_cut_df[i] = kept_data\n",
    "        \n",
    "        \n",
    "#Cut 3: Fractional  𝑝𝑡  difference (between  𝐸𝑚𝑖𝑠𝑠  + jets and leptons) below 0.2\n",
    "desired_indexes = []\n",
    "for i in range(len(delR_cut_df['met_et'])):\n",
    "    ptMJ = (delR_cut_df['met_et'][i] + np.sum(delR_cut_df['jet_pt'][i]))/1000 \n",
    "    #is met_et right variable, or do I need an extra calc? also divide by 1000 to go from MeV to GeV\n",
    "    ptL = np.sum(delR_cut_df['lep_pt'][i])/1000 #also divide by 1000 to go from MeV to GeV \n",
    "    if ((ptMJ - ptL)/ptL) < 0.2:\n",
    "        desired_indexes.append(i)\n",
    "fracpt_cut_df = pd.DataFrame()#make a new dataframe to hold data after the cut\n",
    "for i in new_relevant_data:\n",
    "    kept_data = []\n",
    "    for j in desired_indexes:\n",
    "        kept_data.append(delR_cut_df[i][j])\n",
    "    fracpt_cut_df[i] = kept_data\n",
    "        \n",
    "        \n",
    "#Cut 4:  𝐸𝑚𝑖𝑠𝑠/𝐻𝑡  greater than 0.6\n",
    "desired_indexes = []\n",
    "for i in range(len(fracpt_cut_df['lep_pt'])):\n",
    "    Ht = np.sum(fracpt_cut_df['jet_pt'][i]) + np.sum(fracpt_cut_df['lep_pt'][i])\n",
    "    if (fracpt_cut_df['met_et'][i]/Ht) > 0.6: # DON'T also divide by 1000 to go from MeV to GeV (why?)\n",
    "        desired_indexes.append(i)\n",
    "EmHt_cut_df = pd.DataFrame()#make a new dataframe to hold data after the cut\n",
    "for i in new_relevant_data:\n",
    "    kept_data = []\n",
    "    for j in desired_indexes:\n",
    "        kept_data.append(fracpt_cut_df[i][j])\n",
    "    EmHt_cut_df[i] = kept_data\n",
    "\n",
    "        \n",
    "#Cut 5: Veto events with 1 or more B-tagged jets      \n",
    "desired_indexes = []\n",
    "for i in range(len(EmHt_cut_df['jet_MV2c10'])):\n",
    "    check_b = []\n",
    "    for j in EmHt_cut_df['jet_MV2c10'][i]:\n",
    "        if j > 0.1758:\n",
    "            check_b.append('veto')\n",
    "    if 'veto' in check_b: #these 4 lines below were previously indented, so gave inaccurate results\n",
    "        continue #actually I don't think it would have made a difference, just checks after each value is b-tagged\n",
    "    else: #actually it does matter, it only doesn't if the first value checked is b-tagged\n",
    "        desired_indexes.append(i)\n",
    "\n",
    "final_df2 = pd.DataFrame()#change this dataframe name when re-running this for files that are grouped\n",
    "for i in new_relevant_data:\n",
    "    kept_data = []\n",
    "    for j in desired_indexes:\n",
    "        kept_data.append(EmHt_cut_df[i][j])\n",
    "    final_df2[i] = kept_data #also change this too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2625f1",
   "metadata": {},
   "source": [
    "#### Combining dataframes that are grouped together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ed903dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_dfs = [final_df,final_df1,final_df2] #write names of dataframes to be combined\n",
    "ready_df = pd.concat(combine_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc52bcb",
   "metadata": {},
   "source": [
    "### Writing to a csv file\n",
    "first we need to open (create) the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4a431a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "make = open('C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\csv\\\\ttV.csv','w')\n",
    "#The below line actually does this, but keep this so you can vlose the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ad3e8f",
   "metadata": {},
   "source": [
    "And then it's as simple as using the .to_csv method from pandas to write into the file above by specifying the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "36dd1d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_df.to_csv(path_or_buf='C:\\\\Users\\\\Chris\\\\Desktop\\\\Internship\\\\Data\\\\csv\\\\ttV.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc13033b",
   "metadata": {},
   "source": [
    "Once the csv has been written to, it then needs to be closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9f579481",
   "metadata": {},
   "outputs": [],
   "source": [
    "make.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf3fcf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d0bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
